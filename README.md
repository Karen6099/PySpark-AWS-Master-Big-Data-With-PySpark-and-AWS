# PySpark-AWS-Master-Big-Data-With-PySpark-and-AWS

This repository contains examples and tutorials for mastering big data processing with PySpark and AWS.
https://www.udemy.com/course/pyspark-aws-master-big-data-with-pyspark-and-aws/
Doing the PySpark course for starting my graduate role in the data science team

## Project Structure

- `notebooks/`: Jupyter notebooks with PySpark examples
- `data/`: Sample data files used in the notebooks
- `scripts/`: Python scripts (e.g., Databricks exports)

## Getting Started

1. Install PySpark: `pip install pyspark`
2. Open the notebooks in Jupyter or VS Code
3. Run the cells to see PySpark in action

## Contents

- **My First Notebook.ipynb**: Basic PySpark setup, reading text files into RDDs, and collecting results.
- **Map.ipynb**: Demonstrates the `map` transformation in PySpark, including reading text files and applying transformations to split text lines.

## Prerequisites

- Java 21
- Python 3.11 (Python 3.14 is not yet fully compatible with PySpark 4.0.1)
- PySpark 4.0.1
- Jupyter Notebook (optional)

**Note:** If using Python 3.14, you may encounter worker process crashes. Use Python 3.11 or 3.12 for stable operation.

## License

This project is for educational purposes.